{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "from sketch2code.data_model import *\n",
    "from sketch2code.datasets import *\n",
    "from sketch2code.helpers import *\n",
    "from sketch2code.methods.lstm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "\n",
    "Use PennTreeBank to solve POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 19 60\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "\n",
    "full_data = [treebank.tagged_sents(x) for x in treebank.fileids()]\n",
    "\n",
    "train_data, test_data = full_data[:int(len(full_data) * 0.7)], full_data[int(len(full_data) * 0.7):]\n",
    "train_data, valid_data = train_data[:-int(len(full_data) * 0.1)], train_data[-int(len(full_data) * 0.1):]\n",
    "\n",
    "print(len(train_data), len(valid_data), len(test_data))\n",
    "\n",
    "def flatten(xs):\n",
    "    return [x for a in xs for x in a]\n",
    "\n",
    "train_data, valid_data, test_data = flatten(train_data), flatten(valid_data), flatten(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make vocabulary and predicted labels\n",
    "vocab = set(w for sent in train_data for w, l in sent)\n",
    "assert '<pad>' not in vocab and '<unknown>' not in vocab\n",
    "vocab = ['<pad>'] + sorted(list(vocab)) + ['<unknown>']\n",
    "vocab_w2i = {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "labels = set(l for sent in train_data for w, l in sent)\n",
    "assert all(l in labels for sent in chain(valid_data, test_data) for w, l in sent)\n",
    "labels = {l: i for i, l in enumerate(['<pad>'] + sorted(list(labels)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2712 2712\n",
      "356 356\n",
      "846 846\n"
     ]
    }
   ],
   "source": [
    "# separate input and label\n",
    "def separate_input_and_label(data):\n",
    "    global vocab_w2i, labels\n",
    "    X = []\n",
    "    Y = []\n",
    "    unknown_id = vocab_w2i['<unknown>']\n",
    "    for sent in data:\n",
    "        X.append([vocab_w2i.get(w, unknown_id) for w, l in sent])\n",
    "        Y.append([labels[l] for w, l in sent])\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "X_train, y_train = separate_input_and_label(train_data)\n",
    "X_valid, y_valid = separate_input_and_label(valid_data)\n",
    "X_test, y_test = separate_input_and_label(test_data)\n",
    "\n",
    "print(len(X_train), len(y_train))\n",
    "print(len(X_valid), len(y_valid))\n",
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, lstm, padding_label_idx: int, n_labels: int):\n",
    "        super().__init__()\n",
    "        self.padding_label_idx = padding_label_idx\n",
    "        self.n_labels = n_labels\n",
    "        self.lstm = lstm\n",
    "        self.hidden2tag = torch.nn.Linear(self.lstm.hidden_size, n_labels)\n",
    "    \n",
    "    def forward(self, X, X_lengths):\n",
    "        batch_size, T = X.shape\n",
    "        \n",
    "        X, hn = self.lstm(X, X_lengths)\n",
    "        X = X.contiguous()\n",
    "        X = X.view(-1, X.shape[2])\n",
    "        X = self.hidden2tag(X)\n",
    "        X = torch.nn.functional.log_softmax(X, dim=1)\n",
    "        \n",
    "        # convert back to batch_size, T, tags\n",
    "        X = X.view(batch_size, T, -1)\n",
    "        return X\n",
    "    \n",
    "    def loss(self, Y_hat, Y, X_lengths):\n",
    "        # flatten Y and create a mask if it is \n",
    "        Y = Y.view(-1)\n",
    "        mask = (Y != self.padding_label_idx).float()\n",
    "        Y_hat = Y_hat.view(-1, self.n_labels)\n",
    "              \n",
    "        Y_hat = Y_hat[range(Y_hat.shape[0]), Y] * mask\n",
    "        # cross entropy loss\n",
    "        ce_loss = -torch.sum(Y_hat) / int(torch.sum(mask).data[0])\n",
    "        \n",
    "        return ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(sents: List[List[int]], sent_lbls: List[List[int]]):\n",
    "    pad_w = vocab_w2i['<pad>']\n",
    "    pad_l = labels['<pad>']\n",
    "    \n",
    "    sentence_index_and_length = sorted([(i, len(s)) for i, s in enumerate(sents)], key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    padded_sents = torch.ones((len(sents), sentence_index_and_length[0][1]), dtype=torch.long) * pad_w\n",
    "    padded_lbls = torch.ones_like(padded_sents) * pad_l\n",
    "    \n",
    "    for i, (j, nw) in enumerate(sentence_index_and_length):\n",
    "        padded_sents[i, :nw] = torch.tensor(sents[j])\n",
    "        padded_lbls[i, :nw] = torch.tensor(sent_lbls[j])\n",
    "    \n",
    "    return padded_sents, padded_lbls, torch.tensor([nw for i, nw in sentence_index_and_length])\n",
    "\n",
    "\n",
    "def iter_batch(batch_size: int, X, y, shuffle: bool=False): \n",
    "    index = range(len(X))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(index)\n",
    "        \n",
    "    for i in range(0, len(X), batch_size):\n",
    "        batch_idx = index[i:i+batch_size]\n",
    "        bx, by, bxlen = prepare_batch([X[j] for j in batch_idx], [y[j] for j in batch_idx])\n",
    "        yield (bx, by, bxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "lstm = LSTM(vocab_size=len(vocab), embedding_dim=20, hidden_size=20, n_layers=1, padding_token_idx=0)\n",
    "model = TestModel(lstm, padding_label_idx=0, n_labels=len(labels))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "136it [00:01, 88.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 validation loss: 2.4000649452209473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "136it [00:01, 90.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 validation loss: 1.552728533744812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "136it [00:01, 91.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 validation loss: 0.9840391278266907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "136it [00:01, 89.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 validation loss: 0.6627480983734131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "136it [00:01, 92.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 validation loss: 0.5016286373138428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "136it [00:01, 90.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 validation loss: 0.41821572184562683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "136it [00:01, 89.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 validation loss: 0.36950239539146423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "136it [00:01, 91.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 validation loss: 0.34019362926483154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "136it [00:01, 92.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 validation loss: 0.3250828981399536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "136it [00:01, 91.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 validation loss: 0.3156919479370117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epoches = 10\n",
    "batch_size = 20\n",
    "\n",
    "loss_func = torch.nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-3)\n",
    "\n",
    "bx_val, by_val, bx_val_len = prepare_batch(X_valid, y_valid)\n",
    "bx_val = bx_val.to(device)\n",
    "by_val = by_val.to(device)\n",
    "bx_val_len = bx_val_len.to(device)\n",
    "\n",
    "for epoch in range(n_epoches):\n",
    "    for bx, by, bxlen in tqdm(iter_batch(batch_size, X_train, y_train)):\n",
    "        model.zero_grad()\n",
    "        by_pred = model(bx.to(device), bxlen.to(device))\n",
    "        loss = loss_func(by_pred.view(-1, len(labels)), by.to(device).view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"Epoch\", epoch, 'validation loss:', loss_func(model(bx_val, bx_val_len).view(-1, len(labels)), by_val.view(-1)).item(), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sketch2code",
   "language": "python",
   "name": "sketch2code"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
